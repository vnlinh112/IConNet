name: torch 12g
batch_size: 2
early_stopping: True
accumulate_grad: True
accumulate_grad_scheduler:
  0: 16
  5: 8
  9: 16
  16: 8
  30: 16
  40: 8
  60: 16
  80: 8
checkpoint_save_top_k: 5
optimizer: RAdam
optimizer_kwargs:
  weight_decay: 0.0001
learning_rate_init: 0.001
lr_scheduler: OneCyleLR
lr_scheduler_kwargs:
  step_size: 40
max_epochs: 500
min_epochs: 2
detect_anomaly: False
accelerator: gpu
devices: 1
num_nodes: 1
num_workers: 8
val_check_interval: 0.5
precision: 32
cross_validation: True
num_folds: 5
random_seed: 42 