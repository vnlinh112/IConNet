name: torch 12g
batch_size: 16
early_stopping: False
accumulate_grad: True
accumulate_grad_scheduler:
  0: 8
  40: 4
  60: 2
swa_lrs: 1e-2
# train_scheduler: [12,24,36,48]
optimizer: RAdam
optimizer_kwargs:
  weight_decay: 0.00001
learning_rate_init: 0.001
lr_scheduler: OneCyleLR
lr_scheduler_kwargs:
  step_size: 40
max_epochs: 100
min_epochs: 2
detect_anomaly: False
accelerator: gpu
devices: 1
num_nodes: 1
num_workers: 16
val_check_interval: 0.5
precision: 32
cross_validation: False
num_folds: 5
random_seed: 42 